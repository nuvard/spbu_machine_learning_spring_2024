{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier, XGBRegressor, XGBRFRegressor\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "SEED = 314159\n",
    "TRAIN_TEST_SPLIT = 0.80\n",
    "\n",
    "data_path = \"D:\\data\\machine_learning\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ансамбли: градиентный бустинг\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv(data_path+'/'+\"winequality-red.csv\")\n",
    "df_major = df[df[\"quality\"].isin([5,6])]\n",
    "print(\"Length of filtered data is\", len(df_major))\n",
    "X = df_major.drop('quality', axis=1)\n",
    "y = df_major['quality']\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=SEED)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Градиентный бустинг"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Градиентный бустинг - довольно мощная метамодель, с огромным количеством параметров и хитростей. Мы сегодня остановимся только на оссновных. Для начала рассмотрим самый стандартный бустинг с использованием деревьев решений (CART). Параметры базовых моделей такие же, как и раньше, но настройка амого бустинга довольно сложна!\n",
    "\n",
    "Важный вопрос при обучении модели - какую функцию ошибок выбрать? Какая задача возникает при обработке датасета с вином?\n",
    "\n",
    "Для того, чтобы оценивать модель, полезны различные метрики - численные характеристики ее качества. При этом бустинги настолько галантны, что предоставляют нам возможность оценивать метрики прямо при обучении. Для этого необходимо задать тип метрики в конструкторе и eval_set при запуске fit()."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = XGBClassifier(\n",
    "    objective=\"binary:logistic\", n_estimators=100, learning_rate=1, seed=SEED,\n",
    "    eval_metric=\"auc\"\n",
    ")\n",
    "fit_params = {\"eval_set\":[(X_train, y_train),(X_test, y_test)], \"verbose\": False}\n",
    "# Add verbose=False to avoid printing out updates with each cycle\n",
    "model.fit(X_train, y_train,\n",
    "            eval_set=[(X_train, y_train),(X_test, y_test)],\n",
    "            verbose=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = model.evals_result()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "error_function = \"auc\"\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(results[\"validation_0\"][error_function], label=\"Training loss\")\n",
    "plt.plot(results[\"validation_1\"][error_function], label=\"Validation loss\")\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Как мы видим, хотя лосс при обучении падал и падал, на валидации метрики перестали улучшаться довольно рано. Это довольно плохой знак. Однако говорит ли это о катастрофической ситуации? Проверим переобучение с помощью кросс-валидации."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(model, X, y, cv=10, scoring=[\"accuracy\"],\n",
    "                            return_train_score=True)\n",
    "print(\"Train F1 is\", cv_results['train_accuracy'].mean())\n",
    "print(\"Test F1 is\", cv_results['test_accuracy'].mean())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Кажется, у нас действительно серьезные проблемы. Попробуем уменьшить скорость обучения.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train and eval model with smaller lr\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Помогло ли это? Попробуем получить результаты лучше, поиграв с параметрами."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = XGBClassifier(\n",
    "    # your params\n",
    "\n",
    ")\n",
    "\n",
    "# train, test and plot results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# your code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Так как параметров довольно много, может быть разумно автоматизировать их поиск."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xgboost_params = {\n",
    "    # set your params range\n",
    "                 }\n",
    "xgboost_best_grid = GridSearchCV(model, xgboost_params,\n",
    "                                 cv=7, n_jobs=-1,\n",
    "                                 return_train_score=True).fit(X_train, y_train,**fit_params)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(xgboost_best_grid.best_params_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Давайте проверим, какую точность мы получим с лучшими параметрами."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train and test model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Границы разбиения\n",
    "\n",
    "Мы можем также , как и раньше, построить границы принятия решений для нашего бустинга. При этом мы даже можем использовать хорошо знакомый нам способ из sklearn. Давайте посмотрим, как будут меняться предсказания для разных параметров."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add plot_boundary method. Do we need to change it?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from xgboost import XGBModel\n",
    "from sklearn.decomposition import PCA\n",
    "pca = # your code\n",
    "# make pca decomposition of data\n",
    "features = [\"x1\", \"x2\"]\n",
    "model = # your model\n",
    "# Add silent=True to avoid printing out updates with each cycle\n",
    "fit_params = # set fit params\n",
    "model.fit(X_train_pca, y_train,\n",
    "            **fit_params)\n",
    "plot_boundary(model, data=X_train_pca, features=features, y=y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Регрессия\n",
    "\n",
    "Конечно, градиентный бустинг удобен не только для классификации, но и регрессии. На ее примере давайте и посмотрим, как бустинг формирует предсказания.\n",
    "Построим простейшую зависимость."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "data = pd.DataFrame()\n",
    "data[\"x\"] = np.linspace(-15, 15, 300)\n",
    "data[\"y\"] = # create your simple sinusoidal-like function\n",
    "sns.scatterplot(x=data[\"x\"], y=data[\"y\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "n_estimators = 4\n",
    "model = XGBRegressor(n_estimators=n_estimators)\n",
    "# Add silent=True to avoid printing out updates with each cycle\n",
    "model.fit(data[\"x\"], data[\"y\"], verbose=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Бустинг состоит из нескольких деревьев. Вопрос: какая операция над индивидуальными предсказаниями нужна, чтобы получить финальный ответ?\n",
    "\n",
    "Проиллюстрируем это."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from scipy.special import logit as inverse_sigmoid\n",
    "\n",
    "booster_ = model.get_booster()\n",
    "\n",
    "# Extract indivudual predictions. You will need to use xgboost.DMatrix(input_data)\n",
    "individual_preds = []\n",
    "for tree_ in booster_:\n",
    "    # your code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для начала выведем индивидуальные предсказания деревьев на график"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(individual_preds), 1)\n",
    "for i, preds in enumerate(individual_preds):\n",
    "    sns.scatterplot(x=data[\"x\"], y=data[\"y\"], ax=ax[i])\n",
    "    sns.lineplot(x=data[\"x\"], y=preds, ax=ax[i], c='g')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь можно получить последовательность предсказаний уже для полной модели"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "individual_preds = # your code\n",
    "\n",
    "fig, ax = plt.subplots(len(individual_preds), 1)\n",
    "for i, preds in enumerate(individual_preds):\n",
    "    sns.scatterplot(x=data[\"x\"], y=data[\"y\"], ax=ax[i])\n",
    "    # your code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Дополнительное задание: Построить индивидуальные предсказания и для классификации. Обратите внимание, что предсказания суммируются до взятия сигмоды. Необходимо сделать следущее:\n",
    "\n",
    "# individual_logits = inverse_sigmoid(individual_preds)\n",
    "# final_logits = indivudual_logits.sum(axis=0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Monotonic\n",
    "В реальных задачах часто бывает так, что функциональная форма приемлемой модели каким-то образом ограничена. Так, часто модель ограничена так, чтобы сохранять монотонность. Вопрос: как математически описвается монотонность?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "data[\"x\"] = np.linspace(-15, 15, 300)\n",
    "data[\"y\"] = # make your trendy data\n",
    "sns.scatterplot(x=data[\"x\"], y=data[\"y\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "n_estimators = 10\n",
    "model = XGBRegressor(n_estimators=n_estimators)\n",
    "# fit-predict for your data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "В XGBoost очень просто обеспечить соблюдение монотонности. Для этого достаточно передать модели словарь с названиями фичей и соответствующим ограничением (+1 - возрастание, -1 - убывание).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "n_estimators = 10\n",
    "model = XGBRegressor(n_estimators=n_estimators)  #add your constraints!\n",
    "# fit-predict for your data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Вопрос: что происходит с деревьями, когда на модель задаются ограничения?\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Дополнительная задача - построить деревья до/после задания ограничения"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MSE vs MAE\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_estimators = 10\n",
    "# make two models\n",
    "# Add silent=True to avoid printing out updates with each cycle\n",
    "model_mae.fit(data[\"x\"], data[\"y\"], verbose=False)\n",
    "model_mse.fit(data[\"x\"], data[\"y\"], verbose=False)\n",
    "fig = plt.figure()\n",
    "sns.scatterplot(x=data[\"x\"], y=data[\"y\"])\n",
    "sns.lineplot(x=data[\"x\"], y=model_mae.predict(data[\"x\"]), c='g')\n",
    "sns.lineplot(x=data[\"x\"], y=model_mse.predict(data[\"x\"]), c='r')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Какие выводы можно сделать о результатах предсказания в зависимости от выбора функции потерь? Что еще нужно посмотреть, чтобы сделать более уверенный вывод?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# your code (optional)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Real world example\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Рассмотрим уже известный нам набор данных о стоимости жилья. Это также задача регрессии, причем тут представлены признаки не только непрерывные, но и дискретные."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv(f\"{data_path}/realestate.txt\", sep=\"\\t\")\n",
    "X = data.drop(\"SalePrice\", axis=1)\n",
    "y = data[[\"SalePrice\"]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=SEED)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Посмотрим, как ведет себя модель c разными настройками"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_estimators = 10\n",
    "# train-test regressor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Мы можем явно задать категориальные фичи. Поменяется ли результат? Как думаете, почему?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "categorical_features=[] # select categorical_features\n",
    "X.loc[:, categorical_features] = X[categorical_features].astype(\"category\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_estimators = 10\n",
    "# train test regressor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interaction constraints\n",
    "Когда глубина дерева больше единицы, многие переменные взаимодействуют исключительно ради минимизации потерь при обучении, и результирующее дерево решений может отражать ложные отношения (шум), а не реальные отношения, которые обобщаются для разных наборов данных. Ограничения взаимодействия признаков позволяют пользователям решать, каким переменным разрешено взаимодействовать, а каким нет.\n",
    "\n",
    "Вопрос: для чего это может быть полезно?\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "constraints = [\n",
    "    #select constraints lists\n",
    "]\n",
    "# model\n",
    "# Add silent=True to avoid printing out updates with each cycle\n",
    "model.fit(X_train, y_train, verbose=False)\n",
    "print(\"Train score is: \", model.score(X_train, y_train))\n",
    "print(\"Test score is: \", model.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "xgboost.plot_tree(model, num_trees=4, ax=ax)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Так, пользователи могут иметь предварительные знания о связях между различными признаками и кодировать их как ограничения во время построения модели. Но есть и некоторые тонкости в определении ограничений.\n",
    "\n",
    "В качестве примера возьмем  набор ограничений [[1, 2], [2, 3, 4]]. Как мы видим, 2 признак появляется в обоих наборах: [1, 2] и [2, 3, 4]. Таким образом, фактический набор признаков, которым разрешено взаимодействовать с 2, равен [1, 3, 4].\n",
    "\n",
    "Пусть корень дерева разделяется именно по 2. Поскольку все его потомки должны иметь возможность взаимодействовать с этим узлом, все 4 признака являются законными кандидатами на разделение на втором уровне. Вопрос: является ли это игнорированием исходных ограничений?\n",
    "\n",
    "В качестве еще одного примера возьмем [[0, 1], [0, 1, 2], [1, 2]]. Давайте рассмотрим все возможные наборы для построения второго уровня.\n",
    "\n",
    "Последний пример следующий: [[0, 1], [0, 2, 3]]. Возьмем 1 в качестве признака на 0 уровне. Какие возможные кандидаты могут быть на 1 уровне? А на 2?\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Промежуток предсказания\n",
    "Вопрос: какие значения будет предсказывать бустинг в случае регрессии? Как вы думаете, что он предскажет для \"трендовых\" данных за пределами обучающего набора? А случайный лес?\n",
    "\n",
    "А что будет предсказываться в случае классификации?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Note. Иллюстрация есть в блоге https://medium.com/gousto-engineering-techbrunch/the-problem-with-gradient-boosting-gradient-boosted-gremlins-a69908dcea94_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Случайный лес\n",
    "Интересная особенность бустинга в том, что его можно настроить как случайный лес (при определенных усилиях). Более того, мы можем прямо в интерфейсе XGBoost построить комбинацию бустинга и случайного леса."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_estimators = 10\n",
    "model = XGBRegressor(\n",
    "    # make ranfom forest from GBoosting\n",
    ")\n",
    "# Add silent=True to avoid printing out updates with each cycle\n",
    "model.fit(X_train, y_train, verbose=False)\n",
    "print(\"Train score is: \", model.score(X_train, y_train))\n",
    "print(\"Test score is: \", model.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "xgboost.plot_tree(model, num_trees=4, ax=ax)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_estimators = 10\n",
    "model = XGBRFRegressor() # your params\n",
    "model.fit(X_train, y_train, verbose=False)\n",
    "print(\"Train score is: \", model.score(X_train, y_train))\n",
    "print(\"Test score is: \", model.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "xgboost.plot_tree(model, num_trees=4, ax=ax)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Сравнимся со случайным лесом из Scikit-learn."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# train test sklearn random forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
