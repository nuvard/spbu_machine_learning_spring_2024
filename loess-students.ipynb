{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import (\n",
    "    HuberRegressor,\n",
    "    LinearRegression,\n",
    "    RANSACRegressor,\n",
    "    TheilSenRegressor,\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from scipy.linalg import qr, pinv\n",
    "from scipy.linalg import solve_triangular\n",
    "\n",
    "\n",
    "np.random.seed(42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOESS\n",
    "Рассмотрим обыкновенную линейную регрессию. ЕЕ формула:\n",
    "\\begin{equation}\n",
    "y_i = f(x_i) + \\epsilon_i,\n",
    "\\end{equation}\n",
    "где\n",
    "\\begin{equation}\n",
    "f(x_i) = \\beta_0 + \\beta_1 \\cdot x_i\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Однако рассмотрим данные следующего вида:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = np.linspace(-15, 15, 300)\n",
    "y = np.sin(x) + np.random.normal(0, 0.2, 300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x, y=y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуем обучить модель линейной модели на этих данных"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Обучите линейную модель и постройте графики"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Кажется, что линейная модель никак не может выявить зависимость в данных. Одно из возможных решений - полиномиальная регрессия. Как думаете, какая степень полинома понадобится?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# обавьте полиномиальные фичи и снова обучите модель"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Кажется, что для таких данных построить единую модель будет чрезвычайно сложно. Как можно решить такую проблему? Одно из решений - каким-то образом взвесить семплы, чтобы семплы ближе к переломам были более важны."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Обучите с более высокой степенью"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Выглядит уже ближе. Однако как бы нам сделать процесс взвешивания автоматическим? Рассмотрим модель взвешенной регрессии"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "f(x_i) = w_i(x_0) \\cdot [\\beta_0 + \\beta_1 \\cdot x_i + \\beta_2 \\cdot x_i^2 + \\dots]\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Обозначим через $x_i$ набор n значений для конкретной переменной и пусть $y_i$ представляет соответствующие целевые значения.\n",
    "\n",
    "2. Найдем ближайшие точки к целевой точке $x_0$. Обозначим это множество $D_0$. (Важно учесть расстояния до них: $d_{i0}$).\n",
    "3. Найдем самое большое расстояние между этими точками. Обозначим эту величину $\\delta(x_0)$\n",
    "\n",
    "4. Вычислим весовую функцию $w_i$ для каждого $x_i \\in D_0 $,  используя следующее соотношение:\n",
    "\\begin{equation}\n",
    "w_i(x_0) = W(\\frac{d_{i0}}{\\delta(x_0)})\n",
    "\\end{equation},\n",
    "где:\n",
    "\\begin{equation}\n",
    "    W(u) =\n",
    "    \\begin{cases}\n",
    "      (1 - u^3)^3, & 0 \\leq u < 1 \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "5. Рассчитаем финальные веса регрессии $\\hat f(x_0)$\n",
    " для $x_0$, используя вычисленные веса:\n",
    "\\begin{equation}\n",
    "\\begin{align}\n",
    "\n",
    "WX\\hat \\beta &= Wy \\\\\n",
    "X'WX\\hat \\beta &= X'Wy \\\\\n",
    "\\hat \\beta &= (X'WX)^{-1}X'Wy\n",
    "\n",
    "\\end{align}\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LOESS сочетает в себе простоту линейной регрессии наименьших квадратов с гибкостью нелинейной регрессии. Как и в случае с ближайшими соседями, для LOESS нужны все обучающие данные каждый раз, когда мы хотим вычислить прогноз.\n",
    "\n",
    "Однако как определить, какие точки использовать для подгонки весов? За это отвечает параметр $bandwith$. Этот параметр играет роль, подобную параметру $\\gamma$ в сглаживании сплайнов: контролирует гибкость нелинейной подгонки. Чем меньше полоса пропускания, тем более локальным и волнистым будет сглаживание; очень большой диапазон приведет к глобальной подгонке данных с использованием всех обучающих наблюдений. Вопрос: В случае трикубического окна к какому решению будет стремиться подгон при устремлении $bandwith$ к бесконечности?\n",
    "\n",
    "Так, можно задать полосу пропускания (bandwith) как отступ по x, тогда в нее попадут все x, такие что: $| x_i - x| < h$. Какие минусы такого способа задания окна?\n",
    "\n",
    "Другой способ - задать ее числом соседей, которые будут использоваться для расчетов. Мы будем использовать именно его.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Реализуем метод локальной регрессии. Это придется делать в несколько этапов. Так, нам необходимо решить уравнение\n",
    "\\begin{equation}\n",
    "\\begin{align}\n",
    "\n",
    "WX\\hat \\beta &= Wy \\\\\n",
    "\n",
    "\\end{align}\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def solve(x, y, W, deg: int = 1):\n",
    "\n",
    "    A = np.vander(x, N=1+deg) # |  X^2 |X | X^0 ... - we construct polynomial features from x\n",
    "\n",
    "    # x is horizontal, since we need to transpose it to get solution\n",
    "    V = # (X^T W)^T X\n",
    "    Y = # ( X^T W ) y\n",
    "    Q, R = qr(V)  # make triangular from matrix to get easier solution -> (X^T W)^T X = QR -> final equation: R beta = Q^T * ( X^T W ) y makes\n",
    "    p = solve_triangular(R, np.matmul(Q.T, Y)) # helps to solve equation with triangular left matrix\n",
    "    return p"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Напишем такую функциюб задающую нашу полосу пропускания и определяющие, кто туда попал:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_span(x: np.array, y: np.array, x_target: float, k: int) -> tuple[np.typing.NDArray, np.typing.NDArray, np.typing.NDArray]:\n",
    "    \"\"\" Returns distances, x in span and y in span.\n",
    "     x is all the x, x_targte - target x, k - number of neighbours to use\n",
    "     \"\"\"\n",
    "    # get list of distances\n",
    "    distance = #\n",
    "    # get sorted distances and corresponding indices (using, for example, np.sort + np.argsort)\n",
    "    sorted_dist = #\n",
    "    ind = #\n",
    "\n",
    "    ind_span = ind[:k]\n",
    "    # we need to use x-s and y-s later in the code, so get them too\n",
    "    x_span = #\n",
    "    y_span = #\n",
    "    # which distance is the lowest?\n",
    "    delta_0 = #\n",
    "    # get normalized distances\n",
    "    u = distance[ind_span] / delta_0\n",
    "    return u, x_span, y_span"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loess_tricub(x, y, k, deg):\n",
    "\n",
    "    y_hat = np.zeros(len(x))\n",
    "    x_space = np.zeros_like(x)\n",
    "\n",
    "    for i, x_target in enumerate(x):\n",
    "        # get span\n",
    "        w = # calculate [w_i] for span\n",
    "\n",
    "        W = # make W diagonal\n",
    "        p = # solve equation\n",
    "        y_hat[i] = np.polyval(p, x_target)\n",
    "        x_space[i] = x_target\n",
    "\n",
    "    return y_hat, x_space\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Построим график предсказаний. При этом проанимируем то, как строятся предсказания для разных k."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make loess regression with small k and plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make loess regression with bigger k and plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Чем больше k, тем более сглаженным получается решение. Однако какое k выбрать? Как сделать так, чтобы для датасетов разного размера k было разумным? Можно, например, использовать пропорцию от размера датасета. Реализуем это."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_k_relative(alpha: float, n: int):\n",
    "    return # create size-adaptive bandwidth\n",
    "\n",
    "def loess_tricub_alpha(x, y, alpha, deg):\n",
    "    # your code\n",
    "    return y_hat, x_space\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make regression and plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Можно ли использовать другие функции в качестве весовых? Да, например, воспользуемся колоковидным ядром:\n",
    "\\begin{equation}\n",
    "w^{(i)}  = \\exp \\left( - \\frac{(x^{(i)} - x)^2}{2 \\tau^2} \\right)\n",
    "\\end{equation}\n",
    "Параметр $\\tau$ контролирует, насколько быстро вес обучающего примера падает с увеличением расстояния до точки x\n",
    " и называется параметром _bandwidth_. Вопрос: Как меняется форма колококола при увеличении  $\\tau$?\n",
    "\n",
    "Вопрос: Является ли это ядро гауссовым?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loess(x, y, kernel, alpha, deg, **kernel_params):\n",
    "    # Your code\n",
    "    return x_space, y_pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tricub_kernel(dist):\n",
    "    return # make tricub kernel for calculated distances\n",
    "\n",
    "def bell_kernel(dist, tau):\n",
    "    return  # make bell kernel for calculated distances\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate tricub loess with kernel as parameter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tau = 0.3\n",
    "#make loess with bell kernel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Раз $tau$ регулирует, какой вклад будут давать дальние элементы, чего не было в обычном трикубическом ядре, нужен ли нам теперь параметр $\\alpha$?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set a as 1. and find tau which gives similar results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Если ошибки имеют симметричное распределение (с длинными хвостами) или есть выбросы, мы можем использовать более робастный LOESS.\n",
    "Определим остатки:\n",
    "\\begin{equation}\n",
    "r_i = y_i - f (x_i)\n",
    "\\end{equation}\n",
    "Используем базовое ядро, заданное биквадратной фкнкцией:\n",
    "\\begin{equation}\n",
    "    B(u, b) =\n",
    "    \\begin{cases}\n",
    "      (1 - u^2)^2, & 0 \\leq u < 1 \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "Пусть $m(r) = median(|r|)$. Дополнительные веса тогда:\n",
    "\\begin{equation}\n",
    "\\gamma_i = B(r_i, 6m)\n",
    "\\end{equation}\n",
    "\n",
    "Финальные веса принимают вид $\\tilde {w_i } = gamma_i * w_i.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bicubic_kernel(dist):\n",
    "    return # create bicubic kernel\n",
    "\n",
    "def get_robust_weights(y, y_pred):\n",
    "    residuals = #\n",
    "    s = #\n",
    "    multiplier = # get weights # we clip to preserve final results bounds\n",
    "    return multiplier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loess_robust(x, y, kernel, alpha, deg, iterations, **kernel_params):\n",
    "    k = get_k_relative(alpha=alpha, n=len(x))\n",
    "    y_pred = np.zeros(len(x))\n",
    "    x_space = np.zeros_like(x)\n",
    "    r_weights = np.ones(len(x))\n",
    "    for iter in range(iterations):\n",
    "        # get loess weights\n",
    "        r_weights = get_robust_weights(y, y_pred)\n",
    "\n",
    "    return x_space, y_pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = np.linspace(-15, 15, 300)\n",
    "y = np.sin(x) + np.random.normal(0, 0.2, 300)\n",
    "y[5:10] -= 0.7\n",
    "y[45:50] += 1.1\n",
    "y[-30] += 1.\n",
    "x[-45:-40] -= 0.1\n",
    "\n",
    "x[45] += 1.1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_est_naive , y_est_naive = loess(x, y, kernel=bell_kernel, alpha=0.05, deg=1, tau=tau)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tau = 0.4\n",
    "\n",
    "x_est, y_est = loess_robust(x, y, kernel=bell_kernel, alpha=0.05, deg=1, iterations=5, tau=tau)\n",
    "sns.lineplot(x=x_est, y=y_est, color=\"magenta\")\n",
    "\n",
    "sns.scatterplot(x=x, y=y, color ='skyblue')\n",
    "\n",
    "print(\"error = \", mean_squared_error(y_true=y, y_pred=y_est))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Как видно, все ядра, которые мы использовали ранее, давали меньшие значения при увеличении расстояния. Однако есть ядро, которое использует равномерное распределение. Давайте его реализуем.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Какому методу эквивалентно использование этого ядра?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Рассмотрим и уже знакомую нам задачу предсказания стоимости."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_path = \"D:\\data\\machine_learning\"\n",
    "data = pd.read_csv(f\"{data_path}/realestate.txt\", sep=\"\\t\")\n",
    "\n",
    "data = data[[\"SalePrice\", \"SqFeet\", \"Lot\"]]\n",
    "data = np.log(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[[\"SqFeet\"]], data[\"SalePrice\"])\n",
    "\n",
    "model = LinearRegression().fit(X_train, y_train)\n",
    "preds = model.predict(X_train)\n",
    "loess_x, loess_y = loess_robust(X_train.values.astype(float).ravel(), y_train.values.astype(float).ravel(), kernel=bell_kernel, alpha=0.05, deg=1, iterations=5, tau=tau)\n",
    "loess_x_naive , loess_y_naive = loess(X_train.values.astype(float).ravel(), y_train.values.astype(float).ravel(), kernel=bell_kernel, alpha=0.05, deg=1, tau=tau)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tau = 0.4\n",
    "\n",
    "print('--')\n",
    "sns.lineplot(x=loess_x, y=loess_y , color=\"magenta\")\n",
    "print('--')\n",
    "sns.lineplot(x=loess_x_naive, y= loess_y_naive , color ='skyblue')\n",
    "sns.lineplot(x=X_train.values.astype(float).ravel(), y= preds.ravel() , color ='lightgreen')\n",
    "sns.scatterplot(x=X_train.values.astype(float).ravel(), y=y_train.values.astype(float).ravel() , color ='skyblue')\n",
    "print(\"Linear Regression error: \", mean_squared_error(y_true=y_train.values.astype(float).ravel(), y_pred=preds))\n",
    "print(\"loess error = \", mean_squared_error(y_true=y_train.values.astype(float).ravel(), y_pred=loess_y_naive))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
